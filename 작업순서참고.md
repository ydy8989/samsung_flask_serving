##  Anaconda  환경에서 작업하였음.



### 1. 가상환경생성

```sh
conda create -n samsung python=3.6 

source activate samsung
```



### 2. Jupyter notebook 실행 

* 제가 준 디렉토리를 압축풀고 그 경로로 이동  ( 주피터 파일을 열어보면 주석처리 하였음. )

```sh
source activate samsung
# 아래 패키지를 설치하는 이유는 주피터 노트북에서 콘다 가상환경 커널로 파이썬을 실행하기 위해서 
conda install nb_conda
# 노트북 실행 
cd /home/samsung/
jupyter notebook
```



### 3.  학습 필요 패키지 및 학습진행

```sh
pip install tensorflow keras flask flask_restful requests
mkdir -p ./output
```

```python
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import sys
import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
import numpy as np
import site 
np.random.seed(7)

print('Python version : ', sys.version)
print('TensorFlow version : ', tf.__version__)
print('Keras version : ', keras.__version__)
print("패키지 경로", site.getsitepackages())


img_rows = 28
img_cols = 28

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

input_shape = (img_rows, img_cols, 1)
x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.

print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

batch_size = 128
num_classes = 10
epochs = 12

y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)


model = Sequential()
model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1), padding='same',
                 activation='relu',
                 input_shape=input_shape))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
model.add(Conv2D(64, (2, 2), activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(1000, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
hist = model.fit(x_train, y_train,
                 batch_size=batch_size,
                 epochs=1,
                 verbose=1, 
                 validation_data=(x_test, y_test))
model.save('./output/model.h5')
```

* pb file Convert   ( 한번 실행하고 다시할경우에는 에러 발생 --> 이미 디렉토리가 존재하기 때문에 )
* input name이나 output네임은 따로 지정해줄 수 있음 ( 현재 보여주기 위해 하나는 지정하고 하나는 그대로 사용하였음.)

```python
import tensorflow as tf
tf.reset_default_graph() 
# The export path contains the name and the version of the model
tf.keras.backend.set_learning_phase(0) # Ignore dropout at inference
model = tf.keras.models.load_model('./output/model.h5')
export_path = './output/1'

# Fetch the Keras session and save the model
# The signature definition is defined by the input and output tensors
# And stored with the default serving key
with tf.keras.backend.get_session() as sess:
    tf.saved_model.simple_save(
        sess,
        export_path,
        inputs={'input_image': model.input},
        outputs={t.name:t for t in model.outputs})
```



### 4. 디렉토리 구조 결과 확인

* home/samsung 
  * output
    * 1
      * variables
      * saved_model.pb
    * model.h5
  * train.ipynb
  * config.py
  * app.py
  * model_config.json
  * one.png
  * six.png
  * test.py



### 5. Tensorflow Serving 패키지 설치 및 실행  ( 리눅스에서 실행할 것 )

* 모델을 업데이트할 경우 자동으로 업데이트 되는 것 확인됨.   (  예를들어 1의 디렉토리를 복사하여 2로 새로 만들어주면 로그에 변화가 생김 )

```sh
pip install grpcio tensorflow-serving-api-python3

## 모델 1개만 띄울 경우 !!! 
tensorflow_model_server --port=9000 --model_name=mnist_keras --model_base_path=/home/samsung/output

```

* config 파일을 이용할 경우!  ( config 파일을 만들어주기 )

```python
### config.py
model_config_list: {
  config: {
    name: "mnist_keras",
    base_path: "/home/samsung/output",
    model_platform: "tensorflow"
  },
  config: {
    name: "mnist_keras2",
    base_path: "/home/samsung/output",
    model_platform: "tensorflow"
  },
}

```

```sh
tensorflow_model_server --port=9000 --model_config_file=/home/samsung/config.py
```



### 6. inputs outputs이름을 알아내기  ( signature_def 의 기본값은 "serving_default")

```sh
## linux 
saved_model_cli show --dir /home/samsung/output/1 \
--tag_set serve --signature_def serving_default 
```

* 결과

```json
The given SavedModel SignatureDef contains the following input(s):
  inputs['input_image'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 28, 28, 1)
      name: conv2d_1_input:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['dense_2/Softmax:0'] tensor_info:
      dtype: DT_FLOAT
      shape: (-1, 10)
      name: dense_2/Softmax:0
Method name is: tensorflow/serving/predict

```



### 7. 테스트를 위한 스크립트 파일 만들기( 추후 flask 만들경우 참조 )

```sh
# 리눅스에서 패키지 설치
pip install opencv-python

```

```python

import numpy as np
import cv2
from grpc.beta import implementations
from tensorflow.contrib.util import make_tensor_proto
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2
import os

channel = implementations.insecure_channel('localhost', 9000)
stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)
request = predict_pb2.PredictRequest()
request.model_spec.name = 'mnist_keras'
request.model_spec.signature_name = 'serving_default'

#### data preprocessing
gray = cv2.imread('/home/samsung/one.png', cv2.IMREAD_GRAYSCALE)
data = gray.astype(np.float32)
data = data[np.newaxis]
data = data[np.newaxis]
train_input = np.reshape(data, (1,28,28,1 ))


request.inputs['input_image'].CopyFrom(make_tensor_proto(train_input, shape=list(train_input.shape)))
result_predict = stub.Predict(request, 1000.0)
print("-----result-----")
print( result_predict)

#### output Type Function ####
def type_val(result, output_name, output_type):
    if output_type =='DT_FLOAT':
        out = result.outputs[output_name].float_val
    elif output_type =='DT_INT64':
        out = result.outputs[output_name].int64_val
    elif output_type =='DT_STRING':
        out = result.outputs[output_name].string_val
        for inx, i in enumerate(out):
            if isinstance(out[inx], bytes):
                out[inx] = i.decode("utf-8")
    else:
        out=1
    return out
 
final = {}
final['dense_2/Softmax:0'] = list(type_val(result_predict, output_name='dense_2/Softmax:0', output_type='DT_FLOAT' ))
print(final)
```

```sh
## 리눅스에서 파이썬 파일 실행
python test.py
```

* 결과

```json
outputs {
  key: "dense_2/Softmax:0"
  value {
    dtype: DT_FLOAT
    tensor_shape {
      dim {
        size: 1
      }
      dim {
        size: 10
      }
    }
    float_val: 0.0
    float_val: 1.0
    float_val: 0.0
    float_val: 0.0
    float_val: 0.0
    float_val: 0.0
    float_val: 0.0
    float_val: 0.0
    float_val: 0.0
    float_val: 0.0
  }
}
model_spec {
  name: "mnist_keras"
  version {
    value: 2
  }
  signature_name: "serving_default"
}

```



### 8. Flask App 만들기

* model_config.json

  ==> 만들어주는 이유 : flask에서 모델정보를 불러서 사용하기 위함. 

  ==> 추후 전처리 방식도 if문을 처리하던지 model_config로 정의하던지 결정할 것  ( 현재는 하나만 생각했음. 이미지 사이즈도 박아넣었음. )

```json
## mode_config.json  ( 이건 파일에 넣으면 에러가 발생하니까 주석은 삭제! )
[ 
    { 
       "model_name":"mnist_keras",
       "inputs":"input_image",
       "outputs":"dense_2/Softmax:0",
       "outputs_type": "DT_FLOAT",
       "signature_name":"serving_default"
    },
    { 
       "model_name":"mnist_keras",
       "inputs":"input_image",
       "outputs":"dense_2/Softmax:0",
       "outputs_type": "DT_FLOAT",
       "signature_name":"serving_default"
    
    }
]
```

* app.py 플라스크 파일 만들기 

```python
from flask import Flask,request
from flask_restful import Resource, Api

from grpc.beta import implementations
from tensorflow.contrib.util import make_tensor_proto
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2

import requests
import argparse
import numpy as np
import json
import os
import cv2

app = Flask(__name__)
api = Api(app)
app.config['JSON_AS_ASCII'] = False


#### Post Input ##
parser = argparse.ArgumentParser()
parser.add_argument("-serving_port", "--serving_port", type=int, default=None,
                help='Serving Port ')
args = parser.parse_args()


# port가 고정되있을 것 ( 임의로 박아넣던가 Flag로 추가하여 포트를 인자로 받는다.)
channel = implementations.insecure_channel('localhost',args.serving_port)
stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)
request_tensor = predict_pb2.PredictRequest()

## 추후에 할 경우에 inputs 랑 outputs네임의 config파일을 만들어서 관리해야 한다.!! 
import json
with open('./model_config.json', 'r') as w:
    json_data = w.read()
    dictionary_data = json.loads(json_data)



#### output Type Function ####
def type_val(result, output_name, output_type):
    if output_type =='DT_FLOAT':
        out = result.outputs[output_name].float_val
    elif output_type =='DT_INT64':
        out = result.outputs[output_name].int64_val
    elif output_type =='DT_STRING':
        out = result.outputs[output_name].string_val
        for inx, i in enumerate(out):
            if isinstance(out[inx], bytes):
                out[inx] = i.decode("utf-8")
    else:
        out=1
    return out



################이부분은 모델마다 전처리가 다를 가능성이 높으므로 if 문으로 처리하던지 할 것 . 
def image_preprocessing(model_name, image_path):
    if model_name =='mnist_keras':
        gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        data = gray.astype(np.float32)
        data = data[np.newaxis]
        data = data[np.newaxis]
        data = np.reshape(data, (1,28,28,1 ))
        return data




class process_inference(Resource):
    def post(self):
        json_data = request.get_json(force=True)
        image_path = json_data['image_path']
        model_name = json_data['model_name']
        for inx, i in enumerate(dictionary_data):
            i['model_name'] == model_name
            break
        model_data = dictionary_data[inx]
        signature_name = model_data['signature_name']
        inputs = model_data['inputs']
        outputs = model_data['outputs']
        outputs_type = model_data['outputs_type']
        image_data = image_preprocessing(model_name, image_path)
        # 예측 !! 
        request_tensor.model_spec.name = model_name
        request_tensor.model_spec.signature_name = signature_name
        request_tensor.inputs[inputs].CopyFrom(make_tensor_proto(image_data, shape=list(image_data.shape)))
        result_predict = stub.Predict(request_tensor, 1000.0)
        print("-----result-----")
        print( result_predict)
        final = {}
        final['결과'] = list(type_val(result_predict, output_name=outputs, output_type=outputs_type ))
        return final

api.add_resource(process_inference,'/interlock/inference')
if __name__ == '__main__':
    app.run(host='0.0.0.0',port=5000,debug=True, threaded=True)
```





### 9. Flask 실행

```sh
## 리눅스에서 실행 
python app2.py --serving_port=9000
```



### 10 . Postman 에서 테스트 ( 없으면 다운받기 )

* 아래사진과 같이 따라할것
* 1) Post로 변경할 것 
* 2) headers 눌러서 아래와 같이 추가할것
* 3) body에서 아래json을 입력할 것 
* 4) send 눌러서 결과 확인 

![1568211130222](https://user-images.githubusercontent.com/49559408/64704685-a1e33000-d4e9-11e9-9bf6-04bf135bf975.png)

![1568211303400](https://user-images.githubusercontent.com/49559408/64704912-00a8a980-d4ea-11e9-8348-ca649b630370.png)



* 결과

![1568211352244](https://user-images.githubusercontent.com/49559408/64705002-2635b300-d4ea-11e9-86e2-b5b4be5ddeec.png)

